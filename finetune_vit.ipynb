{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_ZA0rEzAHD-"
      },
      "source": [
        "\n",
        "\n",
        "I am using a Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224 and use the famous Food101 dataset for finetuning process\n",
        "\n",
        "This model has a size of 346 MB on disk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NxYkp8KXatiI"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet transformers accelerate evaluate datasets peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AppXoIXQANgD"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"google/vit-base-patch16-224-in21k\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Wbj8GUpBAcUw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from peft import PeftModel,LoraConfig, get_peft_model\n",
        "\n",
        "#display the model size\n",
        "def get_model_size(path):\n",
        "  size = 0\n",
        "  for f in os.scandir(path):\n",
        "    size += os.path.getsize(f)\n",
        "\n",
        "    print(f\"Model size: {(size / 1e6):.2} MB\")\n",
        "\n",
        "\n",
        "#display how many parameteres to train\n",
        "def print_trainable_parameters(model, label):\n",
        "    parameters, trainable = 0, 0\n",
        "\n",
        "    for _, p in model.named_parameters():\n",
        "        parameters += p.numel()\n",
        "        trainable += p.numel() if p.requires_grad else 0\n",
        "\n",
        "    print(f\"{label} trainable parameters: {trainable:,}/{parameters:,} ({100 * trainable / parameters:.2f}%)\")\n",
        "\n",
        "#splits the dataset\n",
        "def split_dataset(dataset):\n",
        "    dataset_splits = dataset.train_test_split(test_size=0.1)\n",
        "    return dataset_splits.values()\n",
        "\n",
        "#creates mapping\n",
        "def create_label_mappings(dataset):\n",
        "    label2id, id2label = dict(), dict()\n",
        "    for i, label in enumerate(dataset.features[\"label\"].names):\n",
        "        label2id[label] = i\n",
        "        id2label[i] = label\n",
        "\n",
        "    return label2id, id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "M_ln9F9WBjsj"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm.notebook import tqdm as notebook_tqdm\n",
        "\n",
        "dataset = load_dataset(\"food101\", split=\"train[:10000]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "AGDfJglHCpqe"
      },
      "outputs": [],
      "source": [
        "dataset_train, dataset_test = split_dataset(dataset)\n",
        "dataset_label2id, dataset_id2label = create_label_mappings(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DgfCsDAEr1K",
        "outputId": "df12d8e4-cbdf-469d-bce1-2ae34b990eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The length of testing dataset is: 1000\n",
            "The length of training dataset is: 9000\n"
          ]
        }
      ],
      "source": [
        "print(f\"The length of testing dataset is: {len(dataset_test)}\")\n",
        "print(f\"The length of training dataset is: {len(dataset_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0ngOheGDloo",
        "outputId": "8f42c84a-8f16-4d69-dbde-89e75f144bd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('apple_pie', 0), ('baby_back_ribs', 1), ('baklava', 2), ('beef_carpaccio', 3), ('beef_tartare', 4)]\n",
            "[(0, 'apple_pie'), (1, 'baby_back_ribs'), (2, 'baklava'), (3, 'beef_carpaccio'), (4, 'beef_tartare')]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(list(dataset_label2id.items())[:5]) , print(list(dataset_id2label.items())[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "s1bcBpDfFrv9"
      },
      "outputs": [],
      "source": [
        "#config\n",
        "\n",
        "config = {\n",
        "    \"model\": {\n",
        "        \"epochs\": 5,\n",
        "        \"path\": \"./vit-model\",\n",
        "        \"train_data\":dataset_train,\n",
        "        \"test_data\":dataset_test,\n",
        "        \"label2id\": dataset_label2id,\n",
        "        \"id2label\": dataset_id2label\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Kyin4yUTHNi_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint, use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "KGbsL3boH3TN"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "preprocess_pipeline = Compose([\n",
        "    Resize(image_processor.size[\"height\"]),\n",
        "    CenterCrop(image_processor.size[\"height\"]),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
        "])\n",
        "\n",
        "def preprocess(batch):\n",
        "    batch[\"pixel_values\"] = [\n",
        "        preprocess_pipeline(image.convert(\"RGB\")) for image in batch[\"image\"]\n",
        "    ]\n",
        "    return batch\n",
        "\n",
        "\n",
        "# Let's set the transform function to every train and test sets\n",
        "for cfg in config.values():\n",
        "    cfg[\"train_data\"].set_transform(preprocess)\n",
        "    cfg[\"test_data\"].set_transform(preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3a1XWuQLfju"
      },
      "source": [
        "### Next process\n",
        "\n",
        "- Define a collate function.\n",
        "\n",
        "- Define an evaluation metric. During training, the model should be evaluated on its prediction accuracy. You should define a compute_metrics function accordingly.\n",
        "\n",
        "- Load a pretrained checkpoint. You need to load a pretrained checkpoint and configure it correctly for training.\n",
        "\n",
        "- Define the training configuration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dN3e0ipQO8I"
      },
      "source": [
        "Batches are coming in as lists of dicts, so you can just unpack + stack those into batch tensors.\n",
        "\n",
        "Since the collate_fn will return a batch dict, you can **unpack the inputs to the model later. *✨*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "bE4EH_WNIce8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "import torch\n",
        "from peft import PeftModel, LoraConfig, get_peft_model\n",
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "def data_collate(examples):\n",
        "    \"\"\"\n",
        "    Prepare a batch of examples from a list of elements of the\n",
        "    train or test datasets.\n",
        "    \"\"\"\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute the model's accuracy on a batch of predictions.\n",
        "    \"\"\"\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
        "\n",
        "\n",
        "def get_base_model(label2id, id2label):\n",
        "    \"\"\"\n",
        "    Create an image classification base model from\n",
        "    the model checkpoint.\n",
        "    \"\"\"\n",
        "    return AutoModelForImageClassification.from_pretrained(\n",
        "        model_checkpoint,\n",
        "        label2id=label2id,\n",
        "        id2label=id2label,\n",
        "        ignore_mismatched_sizes=True,\n",
        "    )\n",
        "\n",
        "def build_lora_model(label2id, id2label):\n",
        "    \"\"\"Build the LoRA model to fine-tune the base model.\"\"\"\n",
        "    model = get_base_model(label2id, id2label)\n",
        "    print_trainable_parameters(model, label=\"Base model\")\n",
        "\n",
        "    config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"query\", \"value\"],\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        modules_to_save=[\"classifier\"],\n",
        "    )\n",
        "\n",
        "    lora_model = get_peft_model(model, config)\n",
        "    print_trainable_parameters(lora_model, label=\"LoRA\")\n",
        "\n",
        "    return lora_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "vgtV4AQGRGxw"
      },
      "outputs": [],
      "source": [
        "#configuring fine-tuning process\n",
        "\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 128\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./model-checkpoints\",\n",
        "    remove_unused_columns=False,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-3,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=4,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    label_names=[\"labels\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "L6ZAWXNNU2_X",
        "outputId": "14c5706e-94da-48a7-dbd2-5145becd7628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
            "Wall time: 7.15 µs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model trainable parameters: 85,876,325/85,876,325 (100.00%)\n",
            "LoRA trainable parameters: 667,493/86,543,818 (0.77%)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [85/85 09:13, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.348200</td>\n",
              "      <td>0.262576</td>\n",
              "      <td>0.937000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.168700</td>\n",
              "      <td>0.213165</td>\n",
              "      <td>0.931000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.087000</td>\n",
              "      <td>0.184241</td>\n",
              "      <td>0.946000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.042100</td>\n",
              "      <td>0.167169</td>\n",
              "      <td>0.948000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:06]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation accuracy: 0.948\n",
            "Model size: 0.00078 MB\n",
            "Model size: 0.0059 MB\n",
            "Model size: 2.7 MB\n",
            "Model size: 2.7 MB\n",
            "Model size: 2.7 MB\n"
          ]
        }
      ],
      "source": [
        "#Let's finetune.\n",
        "%time\n",
        "from transformers import Trainer\n",
        "\n",
        "for cfg in config.values():\n",
        "    training_arguments.num_train_epochs = cfg[\"epochs\"]\n",
        "\n",
        "    trainer = Trainer(\n",
        "        build_lora_model(cfg[\"label2id\"], cfg[\"id2label\"]),\n",
        "        training_arguments,\n",
        "        train_dataset=cfg[\"train_data\"],\n",
        "        eval_dataset=cfg[\"test_data\"],\n",
        "        tokenizer=image_processor,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=data_collate,\n",
        "    )\n",
        "\n",
        "    results = trainer.train()\n",
        "    evaluation_results = trainer.evaluate(cfg['test_data'])\n",
        "    print(f\"Evaluation accuracy: {evaluation_results['eval_accuracy']}\")\n",
        "\n",
        "    # We can now save the fine-tuned model to disk.\n",
        "    trainer.save_model(cfg[\"path\"])\n",
        "    get_model_size(cfg[\"path\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "q0rN8mW5IWrR"
      },
      "outputs": [],
      "source": [
        "def build_inference_model(label2id, id2label, lora_adapter_path):\n",
        "    \"\"\"Build the model that will be use to run inference.\"\"\"\n",
        "\n",
        "    # Let's load the base model\n",
        "    model = get_base_model(label2id, id2label)\n",
        "\n",
        "    # Now, we can create the inference model combining the base model\n",
        "    # with the fine-tuned LoRA adapter.\n",
        "    return PeftModel.from_pretrained(model, lora_adapter_path)\n",
        "\n",
        "\n",
        "def predict(image, model, image_processor):\n",
        "    \"\"\"Predict the class represented by the supplied image.\"\"\"\n",
        "\n",
        "    encoding = image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoding)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    class_index = logits.argmax(-1).item()\n",
        "    return model.config.id2label[class_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJPl2SXfGJ6S",
        "outputId": "e20f70fc-79d3-4867-b32a-14259b367a15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "for cfg in config.values():\n",
        "    cfg[\"inference_model\"] = build_inference_model(cfg[\"label2id\"], cfg[\"id2label\"], cfg[\"path\"])\n",
        "    cfg[\"image_processor\"] = AutoImageProcessor.from_pretrained(cfg[\"path\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "q5z1ub2oJWHm"
      },
      "outputs": [],
      "source": [
        "samples = [\n",
        "    {\n",
        "        \"image\": \"https://www.allrecipes.com/thmb/AtViolcfVtInHgq_mRtv4tPZASQ=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/ALR-187822-baked-chicken-wings-4x3-5c7b4624c8554f3da5aabb7d3a91a209.jpg\",\n",
        "        \"model\": \"model\",\n",
        "    },\n",
        "    {\n",
        "        \"image\": \"https://www.simplyrecipes.com/thmb/KE6iMblr3R2Db6oE8HdyVsFSj2A=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/__opt__aboutcom__coeus__resources__content_migration__simply_recipes__uploads__2019__09__easy-pepperoni-pizza-lead-3-1024x682-583b275444104ef189d693a64df625da.jpg\",\n",
        "        \"model\": \"model\"\n",
        "    }]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5uVQ3H7Jtp1",
        "outputId": "1649cde7-c655-4aa9-9e9c-b23a63697cd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: chicken_wings\n",
            "Prediction: pizza\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "for sample in samples:\n",
        "    image = Image.open(requests.get(sample[\"image\"], stream=True).raw)\n",
        "\n",
        "    inference_model = config[sample[\"model\"]][\"inference_model\"]\n",
        "    image_processor = config[sample[\"model\"]][\"image_processor\"]\n",
        "\n",
        "    prediction = predict(image, inference_model, image_processor)\n",
        "    print(f\"Prediction: {prediction}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_Lrn-17SNoD",
        "outputId": "3f4a9cf1-1cc9-4268-90b2-8ddae05a7074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: vit-model/ (stored 0%)\n",
            "  adding: vit-model/adapter_config.json (deflated 52%)\n",
            "  adding: vit-model/README.md (deflated 66%)\n",
            "  adding: vit-model/adapter_model.safetensors (deflated 8%)\n",
            "  adding: vit-model/preprocessor_config.json (deflated 46%)\n",
            "  adding: vit-model/training_args.bin (deflated 52%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r vit-model.zip ./vit-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "rfq0GzJkSa3x",
        "outputId": "7e869ecb-882d-4a18-f337-75ed1cb79803"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_009f0bd8-d44e-4c48-9a40-85d72a51b45a\", \"vit-model.zip\", 2473212)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('vit-model.zip')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cudavenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
